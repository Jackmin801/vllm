# Fused MoE+LoRA Kernel Benchmark Results

**GPU**: NVIDIA B200  
**Config**: E=8, top_k=2, K=4096, N=4096, max_loras=4, rank=16, slices=1, dtype=bf16  

| Tokens | Base (ms) | Separate (ms) | Fused (ms) | Sep OH (ms) | Fused OH (ms) | Speedup |
|-------:|----------:|--------------:|-----------:|------------:|--------------:|--------:|
| 1 | 0.084 | 0.306 | 0.179 | +0.222 | +0.095 | 1.71x |
| 2 | 0.104 | 0.238 | 0.208 | +0.134 | +0.105 | 1.14x |
| 4 | 0.285 | 0.463 | 0.322 | +0.178 | +0.037 | 1.44x |
| 8 | 0.122 | 0.274 | 0.364 | +0.152 | +0.241 | 0.75x |
| 16 | 0.133 | 0.232 | 0.466 | +0.098 | +0.332 | 0.50x |
| 32 | 0.137 | 0.236 | 0.476 | +0.099 | +0.339 | 0.50x |
| 64 | 0.194 | 0.296 | 0.587 | +0.102 | +0.393 | 0.50x |
| 128 | 0.311 | 0.426 | 0.644 | +0.115 | +0.333 | 0.66x |
| 256 | 0.492 | 0.631 | 0.946 | +0.139 | +0.454 | 0.67x |
| 512 | 0.843 | 1.055 | 1.367 | +0.212 | +0.524 | 0.77x |
| 1,024 | 1.594 | 2.029 | 2.342 | +0.435 | +0.748 | 0.87x |
| 2,048 | 3.083 | 3.886 | 4.277 | +0.803 | +1.195 | 0.91x |
| 4,096 | 6.092 | 7.655 | 8.085 | +1.563 | +1.994 | 0.95x |
| 8,192 | 12.034 | 15.146 | 15.730 | +3.112 | +3.696 | 0.96x |
| 16,384 | 23.914 | 30.106 | 31.049 | +6.192 | +7.135 | 0.97x |
| 32,768 | 47.710 | 60.057 | 61.741 | +12.346 | +14.031 | 0.97x |
| 65,536 | 95.288 | 119.954 | 122.833 | +24.666 | +27.545 | 0.98x |

### Legend

- **Base** — base expert GEMM only (no LoRA)
- **Separate** — base GEMM + separate LoRA shrink + expand kernels
- **Fused** — single fused GEMM+LoRA kernel
- **Sep OH** — overhead of separate LoRA over base (Separate − Base)
- **Fused OH** — overhead of fused LoRA over base (Fused − Base)
- **Speedup** — Separate / Fused (overall, >1 means fused wins)
