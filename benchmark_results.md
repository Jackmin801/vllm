# Fused MoE+LoRA Kernel Benchmark Results

**GPU**: NVIDIA B200  
**Config**: E=8, top_k=2, K=4096, N=4096, max_loras=4, rank=16, slices=1, dtype=bf16  

| Tokens | Base (ms) | Separate (ms) | Fused (ms) | Sep OH (ms) | Fused OH (ms) | Speedup |
|-------:|----------:|--------------:|-----------:|------------:|--------------:|--------:|
| 1 | 0.106 | 0.474 | 0.257 | +0.368 | +0.151 | 1.84x |
| 2 | 0.151 | 0.294 | 0.244 | +0.143 | +0.093 | 1.20x |
| 4 | 0.288 | 0.381 | 0.360 | +0.093 | +0.072 | 1.06x |
| 8 | 0.296 | 0.413 | 0.564 | +0.117 | +0.268 | 0.73x |
| 16 | 0.186 | 0.297 | 0.668 | +0.111 | +0.482 | 0.45x |
| 32 | 0.213 | 0.334 | 0.705 | +0.122 | +0.492 | 0.47x |
| 64 | 0.283 | 0.422 | 0.840 | +0.139 | +0.557 | 0.50x |
| 128 | 0.441 | 0.629 | 0.942 | +0.188 | +0.501 | 0.67x |
| 256 | 1.036 | 1.329 | 1.178 | +0.293 | +0.141 | 1.13x |
| 512 | 1.531 | 2.037 | 1.737 | +0.507 | +0.207 | 1.17x |
| 1,024 | 2.632 | 3.647 | 3.184 | +1.015 | +0.551 | 1.15x |
| 2,048 | 5.050 | 7.039 | 5.621 | +1.989 | +0.571 | 1.25x |
| 4,096 | 9.681 | 13.630 | 10.607 | +3.949 | +0.925 | 1.29x |
| 8,192 | 19.074 | 26.933 | 20.547 | +7.858 | +1.472 | 1.31x |
| 16,384 | 37.880 | 53.562 | 40.694 | +15.682 | +2.814 | 1.32x |
| 32,768 | 75.708 | 106.971 | 80.946 | +31.263 | +5.238 | 1.32x |
| 65,536 | 151.233 | 214.021 | 161.697 | +62.788 | +10.464 | 1.32x |

### Legend

- **Base** — base expert GEMM only (no LoRA)
- **Separate** — base GEMM + separate LoRA shrink + expand kernels
- **Fused** — single fused GEMM+LoRA kernel
- **Sep OH** — overhead of separate LoRA over base (Separate − Base)
- **Fused OH** — overhead of fused LoRA over base (Fused − Base)
- **Speedup** — Separate / Fused (overall, >1 means fused wins)
