# Fused MoE+LoRA Kernel Benchmark Results

**GPU**: NVIDIA B200  
**Config**: E=8, top_k=2, K=4096, N=4096, max_loras=4, rank=16, slices=1, dtype=bf16  

| Tokens | Base (ms) | Separate (ms) | Fused (ms) | Sep OH (ms) | Fused OH (ms) | Speedup |
|-------:|----------:|--------------:|-----------:|------------:|--------------:|--------:|
| 1 | 0.070 | 0.287 | 0.170 | +0.217 | +0.100 | 1.69x |
| 2 | 0.151 | 0.243 | 0.203 | +0.092 | +0.052 | 1.20x |
| 4 | 0.288 | 0.381 | 0.224 | +0.093 | -0.064 | 1.70x |
| 8 | 0.295 | 0.387 | 0.337 | +0.092 | +0.042 | 1.15x |
| 16 | 0.186 | 0.297 | 0.457 | +0.110 | +0.271 | 0.65x |
| 32 | 0.213 | 0.333 | 0.467 | +0.120 | +0.254 | 0.71x |
| 64 | 0.283 | 0.422 | 0.588 | +0.139 | +0.305 | 0.72x |
| 128 | 0.441 | 0.629 | 0.637 | +0.188 | +0.197 | 0.99x |
| 256 | 1.048 | 1.319 | 0.936 | +0.271 | -0.112 | 1.41x |
| 512 | 1.543 | 2.056 | 1.362 | +0.512 | -0.181 | 1.51x |
| 1,024 | 2.638 | 3.652 | 2.337 | +1.014 | -0.301 | 1.56x |
| 2,048 | 5.050 | 7.052 | 4.251 | +2.002 | -0.799 | 1.66x |
| 4,096 | 9.704 | 13.653 | 8.078 | +3.949 | -1.626 | 1.69x |
| 8,192 | 19.102 | 26.984 | 15.725 | +7.882 | -3.377 | 1.72x |
| 16,384 | 37.960 | 53.676 | 31.043 | +15.717 | -6.917 | 1.73x |
| 32,768 | 75.699 | 107.123 | 61.695 | +31.424 | -14.003 | 1.74x |
| 65,536 | 151.274 | 214.058 | 122.837 | +62.783 | -28.437 | 1.74x |

### Legend

- **Base** — base expert GEMM only (no LoRA)
- **Separate** — base GEMM + separate LoRA shrink + expand kernels
- **Fused** — single fused GEMM+LoRA kernel
- **Sep OH** — overhead of separate LoRA over base (Separate − Base)
- **Fused OH** — overhead of fused LoRA over base (Fused − Base)
- **Speedup** — Separate / Fused (overall, >1 means fused wins)
